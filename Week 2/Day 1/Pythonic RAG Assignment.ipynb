{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your First RAG Application\n",
    "\n",
    "In this notebook, we'll walk you through each of the components that are involved in a simple RAG application. \n",
    "\n",
    "We won't be leveraging any fancy tools, just the OpenAI Python SDK, Numpy, and some classic Python.\n",
    "\n",
    "> NOTE: This was done with Python 3.11.4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents:\n",
    "\n",
    "- Task 1: Imports and Utilities\n",
    "- Task 2: Documents\n",
    "- Task 3: Embeddings and Vectors\n",
    "- Task 4: Prompts\n",
    "- Task 5: Retrieval Augmented Generation\n",
    "- Task 6: Visibility Tooling\n",
    "- Task 7: RAG Evaluation Using GPT-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a rather complicated looking visual representation of a basic RAG application.\n",
    "\n",
    "<img src=\"https://i.imgur.com/PvlaIUO.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Imports and Utility \n",
    "\n",
    "We're just doing some imports and enabling `async` to work within the Jupyter environment here, nothing too crazy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q -U numpy matplotlib plotly pandas scipy scikit-learn openai python-dotenv PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aimakerspace.text_utils import TextFileLoader, CharacterTextSplitter\n",
    "from aimakerspace.vectordatabase import VectorDatabase\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Documents\n",
    "\n",
    "We'll be concerning ourselves with this part of the flow in the following section:\n",
    "\n",
    "<img src=\"https://i.imgur.com/jTm9gjk.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Source Documents\n",
    "\n",
    "So, first things first, we need some documents to work with. \n",
    "\n",
    "While we could work directly with the `.txt` files (or whatever file-types you wanted to extend this to) we can instead do some batch processing of those documents at the beginning in order to store them in a more machine compatible format. \n",
    "\n",
    "In this case, we're going to parse our text file into a single document in memory.\n",
    "\n",
    "Let's look at the relevant bits of the `TextFileLoader` class:\n",
    "\n",
    "```python\n",
    "def load_file(self):\n",
    "        with open(self.path, \"r\", encoding=self.encoding) as f:\n",
    "            self.documents.append(f.read())\n",
    "```\n",
    "\n",
    "We're simply loading the document using the built in `open` method, and storing that output in our `self.documents` list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_loader = TextFileLoader(\"data/Tinybird\")\n",
    "documents = text_loader.load_documents()\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/04/2024, 14:44 Kafka Connector · Tinybird Docs\n",
      "https://www.tinybird.co/docs/ingest/kafka 1/6Kafka Connector\n",
      "The Kafka Connector allows you to ingest data from your existing Kafka cluster into Tinybird.\n",
      "The Kafka Connector is fully managed and requires no additional tooling. Connect Tinybird to\n",
      "your Kafka cluster, choose a topic, and Tinybird automatically begins consuming messages from\n",
      "Kafka.\n",
      "Note that you need to grant READ permissions to both the Topic and the Consumer Group to\n",
      "ingest data from Kafka into Tinybird.\n",
      "Using .datasource files\n",
      "If you are managing your Tinybird resources in fil\n"
     ]
    }
   ],
   "source": [
    "print(documents[0][:600])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Text Into Chunks\n",
    "\n",
    "As we can see, there is one document - and it's the entire text of Frakenstein\n",
    "\n",
    "We'll want to chunk the document into smaller parts so it's easier to pass the most relevant snippets to the LLM. \n",
    "\n",
    "There is no fixed way to split/chunk documents - and you'll need to rely on some intuition as well as knowing your data *very* well in order to build the most robust system.\n",
    "\n",
    "For this toy example, we'll just split blindly on length. \n",
    "\n",
    ">There's an opportunity to clear up some terminology here, for this course we will be stick to the following: \n",
    ">\n",
    ">- \"source documents\" : The `.txt`, `.pdf`, `.html`, ..., files that make up the files and information we start with in its raw format\n",
    ">- \"document(s)\" : single (or more) text object(s)\n",
    ">- \"corpus\" : the combination of all of our documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a peek visually at what we're doing here - and why it might be useful:\n",
    "\n",
    "<img src=\"https://i.imgur.com/rtM6Ci6.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see (though it's not specifically true in this toy example) the idea of splitting documents is to break them into managable sized chunks that retain the most relevant local context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "944"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter()\n",
    "split_documents = text_splitter.split_texts(documents)\n",
    "len(split_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at some of the documents we've managed to split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['15/04/2024, 14:44 Kafka Connector · Tinybird Docs\\nhttps://www.tinybird.co/docs/ingest/kafka 1/6Kafka Connector\\nThe Kafka Connector allows you to ingest data from your existing Kafka cluster into Tinybird.\\nThe Kafka Connector is fully managed and requires no additional tooling. Connect Tinybird to\\nyour Kafka cluster, choose a topic, and Tinybird automatically begins consuming messages from\\nKafka.\\nNote that you need to grant READ permissions to both the Topic and the Consumer Group to\\ningest data from Kafka into Tinybird.\\nUsing .datasource files\\nIf you are managing your Tinybird resources in files, there are several settings available to\\nconfigure the Kafka Connector in .datasource  files.\\nKAFKA_CONNECTION_NAME \\x00 The name of the configured Kafka connection in Tinybird\\nKAFKA_BOOTSTRAP_SERVERS \\x00 A comma-separated list of one or more Kafka brokers\\n(including Port numbers)\\nKAFKA_KEY\\x00 The key used to authenticate with Kafka, sometimes called Key, Client Key, or\\nUsername, depending on the']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_documents[0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Embeddings and Vectors\n",
    "\n",
    "Next, we have to convert our corpus into a \"machine readable\" format. \n",
    "\n",
    "Loosely, this means turning the text into numbers. \n",
    "\n",
    "There are plenty of resources that talk about this process in great detail - I'll leave this [blog](https://txt.cohere.com/sentence-word-embeddings/) from Cohere.AI as a resource if you want to deep dive a bit. \n",
    "\n",
    "Today, we're going to talk about the actual process of creating, and then storing, these embeddings, and how we can leverage that to intelligently add context to our queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this is all baked into 1 call - let's look at some of the code that powers this process:\n",
    "\n",
    "Let's look at our `VectorDatabase().__init__()`:\n",
    "\n",
    "```python\n",
    "def __init__(self, embedding_model: EmbeddingModel = None):\n",
    "        self.vectors = defaultdict(np.array)\n",
    "        self.embedding_model = embedding_model or EmbeddingModel()\n",
    "```\n",
    "\n",
    "As you can see - our vectors are merely stored as a dictionary of `np.array` objects.\n",
    "\n",
    "Secondly, our `VectorDatabase()` has a default `EmbeddingModel()` which is a wrapper for OpenAI's `text-embedding-3-small` model. \n",
    "\n",
    "> **Quick Info About `text-embedding-3-small`**:\n",
    "> - It has a context window of **8191** tokens\n",
    "> - It returns vectors with dimension **1536**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ❓QUESTION:\n",
    "\n",
    "Though 1536 is the default dimension of `text-embedding-3-small`, you can pass a dimension parameter and shorten the vector - what method does OpenAI use to achieve this shortening?\n",
    "\n",
    "Truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from getpass import getpass\n",
    "\n",
    "openai.api_key = getpass(\"OpenAI API Key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai.api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call the `async_get_embeddings` method of our `EmbeddingModel()` on a list of `str` and receive a list of `float` back!\n",
    "\n",
    "```python\n",
    "async def async_get_embeddings(self, list_of_text: List[str]) -> List[List[float]]:\n",
    "        return await aget_embeddings(\n",
    "            list_of_text=list_of_text, engine=self.embeddings_model_name\n",
    "        )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cast those to `np.array` when we build our `VectorDatabase()`:\n",
    "\n",
    "```python\n",
    "async def abuild_from_list(self, list_of_text: List[str]) -> \"VectorDatabase\":\n",
    "        embeddings = await self.embedding_model.async_get_embeddings(list_of_text)\n",
    "        for text, embedding in zip(list_of_text, embeddings):\n",
    "            self.insert(text, np.array(embedding))\n",
    "        return self\n",
    "```\n",
    "\n",
    "And that's all we need to do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db = VectorDatabase()\n",
    "vector_db = asyncio.run(vector_db.abuild_from_list(split_documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ❓QUESTION:\n",
    "\n",
    "What advantage does an async implementation provide for us?\n",
    "\n",
    "Non-blocking operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, to review what we've done so far in natural language:\n",
    "\n",
    "1. We load source documents\n",
    "2. We split those source documents into smaller chunks (documents)\n",
    "3. We send each of those documents to the `text-embedding-3-small` OpenAI API endpoint\n",
    "4. We store each of the text representations with the vector representations as keys/values in a dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Similarity\n",
    "\n",
    "The next step is to be able to query our `VectorDatabase()` with a `str` and have it return to us vectors and text that is most relevant from our corpus. \n",
    "\n",
    "We're going to use the following process to achieve this in our toy example:\n",
    "\n",
    "1. We need to embed our query with the same `EmbeddingModel()` as we used to construct our `VectorDatabase()`\n",
    "2. We loop through every vector in our `VectorDatabase()` and use a distance measure to compare how related they are\n",
    "3. We return a list of the top `k` closest vectors, with their text representations\n",
    "\n",
    "There's some very heavy optimization that can be done at each of these steps - but let's just focus on the basic pattern in this notebook.\n",
    "\n",
    "> We are using [cosine similarity](https://www.engati.com/glossary/cosine-similarity) as a distance metric in this example - but there are many many distance metrics you could use - like [these](https://flavien-vidal.medium.com/similarity-distances-for-natural-language-processing-16f63cd5ba55)\n",
    "\n",
    "> We are using a rather inefficient way of calculating relative distance between the query vector and all other vectors - there are more advanced approaches that are much more efficient, like [ANN](https://towardsdatascience.com/comprehensive-guide-to-approximate-nearest-neighbors-algorithms-8b94f057d6b6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"dramatically. The original endpoint query\\nprocessed 5.4 MB of data and took, on average, 40ms to run (which is already pretty fast... this\\nis a simple use case).\\nWith the Materialized View, we get the exact same results, but process only 140kB of data \\x0040x\\nsmaller) in an average of 20 ms. In other words, twice as fast at a fraction of the cost. What's\\nnot to love? 15/04/2024, 14:44 Materialized Views · Tinybird Docs\\nhttps://www.tinybird.co/docs/publish/materialized-views 9/18\\nPointing the endpoint at the new node\\nNow that we've seen how much the performance of the endpoint query has improved by using a\\nMaterialized View, we can easily change which node the endpoint uses. Just select the node\\ndropdown, and choose the new node we created querying the Materialized View. 15/04/2024, 14:44 Materialized Views · Tinybird Docs\\nhttps://www.tinybird.co/docs/publish/materialized-views 10/18\\nThis way, we improve the endpoint performance while retaining the original endpoint URL, so\\napplications\",\n",
       "  0.531350920944138),\n",
       " ('aterialized Views · Tinybird Docs\\nhttps://www.tinybird.co/docs/publish/materialized-views 10/18\\nThis way, we improve the endpoint performance while retaining the original endpoint URL, so\\napplications which call that endpoint will see an immediate performance boost.\\nA practical example using the CLI\\nWe have an events Data Source which for each action performed in an ecommerce website,\\nstores a timestamp, the user that performed the action, the product, which type of action (buy,\\nadd to cart , view, etc.) and a json column containing some metadata, such as the price.\\nIn this guide we will learn how to use the CLI to create Pipes and Materialized Views.\\nThe events data source is expected to store billions of rows per month. Its data schema is as\\nfollows: 15/04/2024, 14:44 Materialized Views · Tinybird Docs\\nhttps://www.tinybird.co/docs/publish/materialized-views 11/18We want to publish an API Endpoint calculating the top 10 products in terms of sales for a date\\nrange ranked by total',\n",
       "  0.4811586341909847),\n",
       " (\" endpoint performance by over 50X.\\nDirectly below, you can read a bit more about why and when to use Materialized Views, or skip\\nstraight to the guide.\\nBefore you get started creating MVs, we strongly recommend that you take some time\\nto read through how they work in Tinybird, since Tinybird takes a unique approach to\\nmaterialization that is different than most databases you may have used.\\nWhy use Materialized Views?\\nWhen you're working in real-time, speed is everything. Queries that try to aggregate or filter\\nover large datasets are doomed to suffer unacceptable levels of latency.\\nMaterialized Views give you a way to pre-aggregate and pre-filter large Data Sources\\nincrementally, adding simple logic using SQL to produce a more relevant Data Source with\\nsignificantly fewer rows.\\nPut simply, Materialized Views shift computational load from query time to ingestion time, so\\nyour endpoints stay fast.\\nWhen should I use Materialized Views?\\nMaterialized Views are used to accomplish 2 main\",\n",
       "  0.4427443696588691)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_db.search_by_text(\"Improve endpoint performance\", k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Prompts\n",
    "\n",
    "In the following section, we'll be looking at the role of prompts - and how they help us to guide our application in the right direction.\n",
    "\n",
    "In this notebook, we're going to rely on the idea of \"zero-shot in-context learning\".\n",
    "\n",
    "This is a lot of words to say: \"We will ask it to perform our desired task in the prompt, and provide no examples.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XYZRolePrompt\n",
    "\n",
    "Before we do that, let's stop and think a bit about how OpenAI's chat models work. \n",
    "\n",
    "We know they have roles - as is indicated in the following API [documentation](https://platform.openai.com/docs/api-reference/chat/create#chat/create-messages)\n",
    "\n",
    "There are three roles, and they function as follows (taken directly from [OpenAI](https://platform.openai.com/docs/guides/gpt/chat-completions-api)): \n",
    "\n",
    "- `{\"role\" : \"system\"}` : The system message helps set the behavior of the assistant. For example, you can modify the personality of the assistant or provide specific instructions about how it should behave throughout the conversation. However note that the system message is optional and the model’s behavior without a system message is likely to be similar to using a generic message such as \"You are a helpful assistant.\"\n",
    "- `{\"role\" : \"user\"}` : The user messages provide requests or comments for the assistant to respond to.\n",
    "- `{\"role\" : \"assistant\"}` : Assistant messages store previous assistant responses, but can also be written by you to give examples of desired behavior.\n",
    "\n",
    "The main idea is this: \n",
    "\n",
    "1. You start with a system message that outlines how the LLM should respond, what kind of behaviours you can expect from it, and more\n",
    "2. Then, you can provide a few examples in the form of \"assistant\"/\"user\" pairs\n",
    "3. Then, you prompt the model with the true \"user\" message.\n",
    "\n",
    "In this example, we'll be forgoing the 2nd step for simplicities sake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility Functions\n",
    "\n",
    "You'll notice that we're using some utility functions from the `aimakerspace` module - let's take a peek at these and see what they're doing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XYZRolePrompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have our `system`, `user`, and `assistant` role prompts. \n",
    "\n",
    "Let's take a peek at what they look like:\n",
    "\n",
    "```python\n",
    "class BasePrompt:\n",
    "    def __init__(self, prompt):\n",
    "        \"\"\"\n",
    "        Initializes the BasePrompt object with a prompt template.\n",
    "\n",
    "        :param prompt: A string that can contain placeholders within curly braces\n",
    "        \"\"\"\n",
    "        self.prompt = prompt\n",
    "        self._pattern = re.compile(r\"\\{([^}]+)\\}\")\n",
    "\n",
    "    def format_prompt(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Formats the prompt string using the keyword arguments provided.\n",
    "\n",
    "        :param kwargs: The values to substitute into the prompt string\n",
    "        :return: The formatted prompt string\n",
    "        \"\"\"\n",
    "        matches = self._pattern.findall(self.prompt)\n",
    "        return self.prompt.format(**{match: kwargs.get(match, \"\") for match in matches})\n",
    "\n",
    "    def get_input_variables(self):\n",
    "        \"\"\"\n",
    "        Gets the list of input variable names from the prompt string.\n",
    "\n",
    "        :return: List of input variable names\n",
    "        \"\"\"\n",
    "        return self._pattern.findall(self.prompt)\n",
    "```\n",
    "\n",
    "Then we have our `RolePrompt` which laser focuses us on the role pattern found in most API endpoints for LLMs.\n",
    "\n",
    "```python\n",
    "class RolePrompt(BasePrompt):\n",
    "    def __init__(self, prompt, role: str):\n",
    "        \"\"\"\n",
    "        Initializes the RolePrompt object with a prompt template and a role.\n",
    "\n",
    "        :param prompt: A string that can contain placeholders within curly braces\n",
    "        :param role: The role for the message ('system', 'user', or 'assistant')\n",
    "        \"\"\"\n",
    "        super().__init__(prompt)\n",
    "        self.role = role\n",
    "\n",
    "    def create_message(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Creates a message dictionary with a role and a formatted message.\n",
    "\n",
    "        :param kwargs: The values to substitute into the prompt string\n",
    "        :return: Dictionary containing the role and the formatted message\n",
    "        \"\"\"\n",
    "        return {\"role\": self.role, \"content\": self.format_prompt(**kwargs)}\n",
    "```\n",
    "\n",
    "We'll look at how the `SystemRolePrompt` is constructed to get a better idea of how that extension works:\n",
    "\n",
    "```python\n",
    "class SystemRolePrompt(RolePrompt):\n",
    "    def __init__(self, prompt: str):\n",
    "        super().__init__(prompt, \"system\")\n",
    "```\n",
    "\n",
    "That pattern is repeated for our `UserRolePrompt` and our `AssistantRolePrompt` as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we have our model, which is converted to a format analagous to libraries like LangChain and LlamaIndex.\n",
    "\n",
    "Let's take a peek at how that is constructed:\n",
    "\n",
    "```python\n",
    "class ChatOpenAI:\n",
    "    def __init__(self, model_name: str = \"gpt-3.5-turbo\"):\n",
    "        self.model_name = model_name\n",
    "        self.openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        if self.openai_api_key is None:\n",
    "            raise ValueError(\"OPENAI_API_KEY is not set\")\n",
    "\n",
    "    def run(self, messages, text_only: bool = True):\n",
    "        if not isinstance(messages, list):\n",
    "            raise ValueError(\"messages must be a list\")\n",
    "\n",
    "        openai.api_key = self.openai_api_key\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=self.model_name, messages=messages\n",
    "        )\n",
    "\n",
    "        if text_only:\n",
    "            return response.choices[0].message.content\n",
    "\n",
    "        return response\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ❓QUESTION:\n",
    "\n",
    "How could we ensure our LLM always responded the same way to our inputs?\n",
    "It is not possible to assure determinist output, even if randomness can be reduced with good model design and better examples CoT or input normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and Prompting OpenAI's `gpt-3.5-turbo`!\n",
    "\n",
    "Let's tie all these together and use it to prompt `gpt-3.5-turbo`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aimakerspace.openai_utils.prompts import (\n",
    "    UserRolePrompt,\n",
    "    SystemRolePrompt,\n",
    "    AssistantRolePrompt,\n",
    ")\n",
    "\n",
    "from aimakerspace.openai_utils.chatmodel import ChatOpenAI\n",
    "\n",
    "chat_openai = ChatOpenAI()\n",
    "user_prompt_template = \"{content}\"\n",
    "user_role_prompt = UserRolePrompt(user_prompt_template)\n",
    "system_prompt_template = (\n",
    "    \"You are an expert in {expertise}, you always answer in a kind way.\"\n",
    ")\n",
    "system_role_prompt = SystemRolePrompt(system_prompt_template)\n",
    "\n",
    "messages = [\n",
    "    user_role_prompt.create_message(\n",
    "        content=\"What is the best way to write a loop?\"\n",
    "    ),\n",
    "    system_role_prompt.create_message(expertise=\"Python\"),\n",
    "]\n",
    "\n",
    "response = chat_openai.run(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best way to write a loop ultimately depends on the specific task you are trying to accomplish. However, in general, it is recommended to keep your loop simple and concise to improve readability and maintainability of your code.\n",
      "\n",
      "Here are some tips for writing a loop effectively:\n",
      "\n",
      "1. Choose the appropriate type of loop for your task: In Python, common loop types include `for` loops and `while` loops. Use a `for` loop when you know the number of iterations in advance, and use a `while` loop when the number of iterations is not known beforehand.\n",
      "\n",
      "2. Ensure the loop has a clear termination condition: Make sure your loop has a clear condition for when it should stop executing to prevent infinite loops.\n",
      "\n",
      "3. Minimize the complexity within the loop: Keep the body of the loop simple and avoid unnecessary code to improve the efficiency and readability of your loop.\n",
      "\n",
      "4. Use meaningful variable names: Choose descriptive variable names that convey the purpose of the loop and its iteration.\n",
      "\n",
      "5. Consider using a list comprehension: In Python, list comprehensions provide a concise and efficient way to create lists using a `for` loop and an optional `if` condition.\n",
      "\n",
      "Overall, the best way to write a loop is to keep it simple, readable, and tailored to the specific requirements of your task. Happy coding!\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Retrieval Augmented Generation\n",
    "\n",
    "Now we can create a RAG prompt - which will help our system behave in a way that makes sense!\n",
    "\n",
    "There is much you could do here, many tweaks and improvements to be made!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_PROMPT_TEMPLATE = \"\"\" \\\n",
    "Use the provided context to answer the user's query. \n",
    "\n",
    "You may not answer the user's query unless there is specific context in the following text.\n",
    "\n",
    "If you do not know the answer, or cannot answer, please respond with \"I don't know\".\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = SystemRolePrompt(RAG_PROMPT_TEMPLATE)\n",
    "\n",
    "USER_PROMPT_TEMPLATE = \"\"\" \\\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "User Query:\n",
    "{user_query}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "user_prompt = UserRolePrompt(USER_PROMPT_TEMPLATE)\n",
    "\n",
    "class RetrievalAugmentedQAPipeline:\n",
    "    def __init__(self, llm: ChatOpenAI(), vector_db_retriever: VectorDatabase) -> None:\n",
    "        self.llm = llm\n",
    "        self.vector_db_retriever = vector_db_retriever\n",
    "\n",
    "    def run_pipeline(self, user_query: str) -> str:\n",
    "        context_list = self.vector_db_retriever.search_by_text(user_query, k=4)\n",
    "        \n",
    "        context_prompt = \"\"\n",
    "        for context in context_list:\n",
    "            context_prompt += context[0] + \"\\n\"\n",
    "\n",
    "        formatted_system_prompt = rag_prompt.create_message()\n",
    "\n",
    "        formatted_user_prompt = user_prompt.create_message(user_query=user_query, context=context_prompt)\n",
    "        \n",
    "        return {\"response\" : self.llm.run([formatted_user_prompt, formatted_system_prompt]), \"context\" : context_list}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ❓QUESTION:\n",
    "\n",
    "What changes could you make that would encourage the LLM to have a more well thought out and verbose response?\n",
    "\n",
    "What is this method called?\n",
    "CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_augmented_qa_pipeline = RetrievalAugmentedQAPipeline(\n",
    "    vector_db_retriever=vector_db,\n",
    "    llm=chat_openai\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response': 'A Materialized View in Tinybird is a combination of a Pipe and a Data Source. Instead of publishing a Node as an API Endpoint, you can write the results of a query into another Data Source. Materialized Views in Tinybird are incremental and triggered upon ingestion, meaning that new rows are automatically processed by the materialization query and combined with the previous result. There is no need for a schedule to refresh data in a Materialized View as they are processed at ingestion time. Materialized Views are beneficial for preprocessing data before using them in API Endpoints, reducing latency and cost-per-query.',\n",
       " 'context': [(' examples of when a Materialized View makes sense:\\nAggregating Sales per Product: You\\'re ingesting real-time sales events data, but you\\nfrequently need to query the total orders or revenue by product.\\nAggregating Truck Routes per Day: You need to save the routes that every truck in your\\nfleet has performed each day. You can create a Materialized View with a groupArray, so you\\ncan easily return the routes per truck per day.\\nError Logs: You work with logs but just want to analyze those with the type \"error\". You can\\ncreate a Materialized View to filter by just that type.\\nReal-Time Events: You have a real-time application that doesn\\'t need historical relevance.\\nYou can create a Materialized View to filter only timestamps within the last minute.\\nChanging Indexes: You want to reuse a Data Source, but you have a new use case that\\nrequires you to filter by non-indexed columns. You can create a Materialized View and index\\nby the columns you want to filter.\\nExtract data from geolocation JSON\\x00',\n",
       "   0.6928363486086783),\n",
       "  ('15/04/2024, 14:46 Materialized Views · Tinybird Docs\\nhttps://www.tinybird.co/docs/concepts/materialized-views 1/4Materialized Views\\nWhat is a Materialized View?\\nA Materialized View in Tinybird is a combination of a Pipe and a Data Source. Instead of\\npublishing a Node as an API Endpoint, you can write the results of a query into another Data\\nSource.\\nUnlike other databases, Materialized Views in Tinybird are incremental and triggered upon\\ningestion. This means that, as you ingest new rows, data is automatically processed by the\\nmaterialization query, combined with the previous result, and written to the Materialized View.\\nBecause of this, there is no schedule to maintain for refreshing data in a Materialized View.\\nMaterialized Views are processed at ingestion time, making them an excellent way to\\npreprocess data before being used in API Endpoints. Materialized Views are critical for reducing\\nboth latency and cost-per-query.\\nWhat should I use Materialized Views for?\\nThere are three common',\n",
       "   0.6788176537381694),\n",
       "  (\"mply, Materialized Views shift computational load from query time to ingestion time, so\\nyour endpoints stay fast.\\nWhen should I use Materialized Views?\\nMaterialized Views are used to accomplish 2 main goals:\\n\\x00\\x00 Improve endpoint performance by reducing query speed and scan size\\n 15/04/2024, 14:44 Materialized Views · Tinybird Docs\\nhttps://www.tinybird.co/docs/publish/materialized-views 2/18\\x00\\x00 Simplifying query development by automating common filters andaggregations\\nAs such, consider using a Materialized View if you are experiencing any of the following:\\nA simple query taking too much time\\nYou're using the same aggregations on the same Data Source in multiple Pipes\\nA single query is processing too much data\\nYou need to change the data schema for a new use case\\nHere are a few real-world examples of when a Materialized View makes sense:\\nAggregating Sales per Product: You're ingesting real-time sales events data, but you\\nfrequently need to query the total orders or revenue by product.\",\n",
       "   0.6764457596657619),\n",
       "  (\"s allow data to be transformed from an origin data source to a destination\\ndata source. There are several scenarios where Materialized Views are really useful and can\\nmake a difference in the response times of your analyses, for instance:\\nDenormalize several normalized tables into one via a JOIN.\\nTransform data using an optimized ENGINE for a concrete analysis.\\nTransform your source data on the fly as you ingest data in your origin data source.\\nIt's important to understand that Materialized Views are live views of your origin data source.\\nAny time you append or replace data to your origin data source, all the destination data\\nsources crated as Materialized Views are properly synced. It means you don't have to worry\\nabout costly re-sync processes.\\nLet's say you have an origin data source (my_origin.datasource ) like this one:    `test3` Int64,\\n    `new_column`  Int64\\nORIGIN DATA SOURCE 15/04/2024, 14:45 CLI common use cases · Tinybird Docs\",\n",
       "   0.6669738706660705)]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_augmented_qa_pipeline.run_pipeline(\"What is a Materialized View?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Visibility Tooling\n",
    "\n",
    "This is great, but what if we wanted to add some visibility to our pipeline?\n",
    "\n",
    "Let's use Weights and Biases as a visibility tool!\n",
    "\n",
    "The first thing we'll need to do is create a Weights and Biases account and get an API key. \n",
    "\n",
    "You can follow the process outlined [here](https://docs.wandb.ai/quickstart) to do exactly that!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can get the Weights and Biases dependency and add our key to our env. to begin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q -U wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_key = getpass(\"Weights and Biases API Key: \")\n",
    "os.environ[\"WANDB_API_KEY\"] = wandb_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkikeap\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/kike/Documents/ai/Kike-AI-Engineering/Week 2/Day 1/wandb/run-20240415_145531-mqiikz4x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kikeap/Visibility%20Example/runs/mqiikz4x' target=\"_blank\">frosty-sun-6</a></strong> to <a href='https://wandb.ai/kikeap/Visibility%20Example' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kikeap/Visibility%20Example' target=\"_blank\">https://wandb.ai/kikeap/Visibility%20Example</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kikeap/Visibility%20Example/runs/mqiikz4x' target=\"_blank\">https://wandb.ai/kikeap/Visibility%20Example/runs/mqiikz4x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/kikeap/Visibility%20Example/runs/mqiikz4x?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1503bb450>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"./Pythonic RAG Assignment.ipynb\"\n",
    "wandb.init(project=\"Visibility Example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can integrate Weights and Biases into our `RetrievalAugmentedQAPipeline`.\n",
    "\n",
    "```python\n",
    "if self.wandb_project:\n",
    "            root_span = Trace(\n",
    "                name=\"root_span\",\n",
    "                kind=\"llm\",\n",
    "                status_code=status,\n",
    "                status_message=status_message,\n",
    "                start_time_ms=start_time,\n",
    "                end_time_ms=end_time,\n",
    "                metadata={\n",
    "                    \"token_usage\" : token_usage\n",
    "                },\n",
    "                inputs= {\"system_prompt\" : formatted_system_prompt, \"user_prompt\" : formatted_user_prompt},\n",
    "                outputs= {\"response\" : response_text}\n",
    "            )\n",
    "\n",
    "            root_span.log(name=\"openai_trace\")\n",
    "```\n",
    "\n",
    "The main things to consider here are how to populate the various fields to make sure we're tracking useful information. \n",
    "\n",
    "We'll use the `text_only` flag to ensure we can get detailed information about our LLM call!\n",
    "\n",
    "You can check out all the parameters for Weights and Biases `Trace` [here](https://github.com/wandb/wandb/blob/653015a014281f45770aaf43627f64d9c4f04a32/wandb/sdk/data_types/trace_tree.py#L166)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from wandb.sdk.data_types.trace_tree import Trace\n",
    "\n",
    "class RetrievalAugmentedGenerationPipeline:\n",
    "    def __init__(self, llm: ChatOpenAI(), vector_db_retriever: VectorDatabase, wandb_project = None) -> None:\n",
    "        self.llm = llm\n",
    "        self.vector_db_retriever = vector_db_retriever\n",
    "        self.wandb_project = wandb_project\n",
    "\n",
    "    def run_pipeline(self, user_query: str) -> str:\n",
    "        context_list = self.vector_db_retriever.search_by_text(user_query, k=4)\n",
    "        \n",
    "        context_prompt = \"\"\n",
    "        for context in context_list:\n",
    "            context_prompt += context[0] + \"\\n\"\n",
    "\n",
    "        formatted_system_prompt = rag_prompt.create_message()\n",
    "\n",
    "        formatted_user_prompt = user_prompt.create_message(user_query=user_query, context=context_prompt)\n",
    "\n",
    "        \n",
    "        start_time = datetime.datetime.now().timestamp() * 1000\n",
    "\n",
    "        try:\n",
    "            openai_response = self.llm.run([formatted_system_prompt, formatted_user_prompt], text_only=False)\n",
    "            end_time = datetime.datetime.now().timestamp() * 1000\n",
    "            status = \"success\"\n",
    "            status_message = (None, )\n",
    "            response_text = openai_response.choices[0].message.content\n",
    "            token_usage = dict(openai_response.usage)\n",
    "            model = openai_response.model\n",
    "\n",
    "        except Exception as e:\n",
    "            end_time = datetime.datetime.now().timestamp() * 1000\n",
    "            status = \"error\"\n",
    "            status_message = str(e)\n",
    "            response_text = \"\"\n",
    "            token_usage = {}\n",
    "            model = \"\"\n",
    "\n",
    "        if self.wandb_project:\n",
    "            root_span = Trace(\n",
    "                name=\"root_span\",\n",
    "                kind=\"llm\",\n",
    "                status_code=status,\n",
    "                status_message=status_message,\n",
    "                start_time_ms=start_time,\n",
    "                end_time_ms=end_time,\n",
    "                metadata={\n",
    "                    \"token_usage\" : token_usage,\n",
    "                    \"model_name\" : model\n",
    "                },\n",
    "                inputs= {\"system_prompt\" : formatted_system_prompt, \"user_prompt\" : formatted_user_prompt},\n",
    "                outputs= {\"response\" : response_text}\n",
    "            )\n",
    "\n",
    "            root_span.log(name=\"openai_trace\")\n",
    "        \n",
    "        return {\"response\" : self.llm.run([formatted_user_prompt, formatted_system_prompt]), \"context\" : context_list} if response_text else \"We ran into an error. Please try again later. Full Error Message: \" + status_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_augmented_qa_pipeline = RetrievalAugmentedGenerationPipeline(\n",
    "    vector_db_retriever=vector_db,\n",
    "    llm=chat_openai,\n",
    "    wandb_project=\"LLM Visibility Example\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response': \"I don't know.\",\n",
       " 'context': [('fig file (~/.zshrc, ~/.bashrc, etc.) and include it in\\nyour PROMPT\\x00\\nOnce the function is available, make the output visible on the PROMPT depends on your shell\\ninstallation, for instance, for the case of zsh this should work in most cases:\\nOnce properly configured, and you are in the root directory of a data project (the one with the\\n.tinyb file), you\\'ll see the Tinybird region and Workspace in your PROMPT\\x00prompt_tb() {\\nif [ -e \".tinyb\"  ]; then\\n    TB_CHAR=$\\'\\\\U1F423\\'\\n    branch_name =`grep \\'\"name\":\\'  .tinyb | cut -d : -f 2 | cut -d \\'\"\\' -f 2`\\n    region=`grep \\'\"host\":\\'  .tinyb | cut -d / -f 3 | cut -d . -f 2 | cut -d :\\n    if [ \"$region\" = \"tinybird\"  ]; then\\n    region=`grep \\'\"host\":\\'  .tinyb | cut -d / -f 3 | cut -d . -f 1`\\n    fi\\n    TB_BRANCH =\"${TB_CHAR} tb:${region} =>${branch_name} \"\\nelse\\n    TB_BRANCH =\\'\\'\\nfi\\necho $TB_BRANCH\\n}PARSE THE .TINYB FILE TO USE THE OUTPUT IN THE PROMPT\\necho \\'export PROMPT=\"\\'  $PS1 \\' $(prompt_tb)\"\\'  >> ~/.zshrcINCLUDE INFO OF THE TINYBIRD CLI IN THE ZSH',\n",
       "   0.11517448386908151),\n",
       "  (' Tinybird. All rights reserved\\nTerms & conditions\\nCookies\\nSecurity',\n",
       "   0.10876486759143174),\n",
       "  ('\\nSecurityDocsGuidesScreencastsChangelogStarter KitsCommunity\\nIntroduction\\nQuick start\\nCore concepts\\nArchitecture\\nStart building\\nSign In',\n",
       "   0.09651852737554185),\n",
       "  ('15/04/2024, 14:47 Auth Tokens · Tinybird Docs\\nhttps://www.tinybird.co/docs/concepts/auth-tokens 1/6Auth Tokens\\nWhat is an Auth Token?\\nAuth Tokens protect access to your Tinybird resources.\\nAny operations to manage your Tinybird resources via the CLI or REST API require a valid Auth\\nToken with the necessary permissions. Access to the APIs you publish in Tinybird are also\\nprotected with Auth Tokens.\\nAuth Tokens can have different scopes. This means you can limit which operations a specific\\nAuth Token can do. You can create Auth Tokens that are, for example, only able to do admin\\noperations on Tinybird resources, or only have READ permission for a specific Data Source.\\nAn Auth Token looks like this:\\nWhat should I use Auth Tokens for?\\nIf you are performing operations on your account (like importing data, creating data sources, or\\npublishing APIs via the CLI or REST API\\x00 you must use an Auth Token.\\nWhen you publish an API that exposes your data to an application, you need an Auth Token to',\n",
       "   0.08942829807145884)]}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_augmented_qa_pipeline.run_pipeline(\"Who is Batman?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response': 'A Materialized View in Tinybird is a combination of a Pipe and a Data Source. Instead of publishing a Node as an API Endpoint, the results of a query are written into another Data Source. Materialized Views in Tinybird are incremental and triggered upon ingestion, meaning as new rows are ingested, data is automatically processed by the materialization query, combined with the previous result, and written to the Materialized View. There is no need to maintain a schedule for refreshing data in a Materialized View. Materialized Views are processed at ingestion time and are useful for preprocessing data before being used in API Endpoints, reducing latency and cost-per-query.',\n",
       " 'context': [(' examples of when a Materialized View makes sense:\\nAggregating Sales per Product: You\\'re ingesting real-time sales events data, but you\\nfrequently need to query the total orders or revenue by product.\\nAggregating Truck Routes per Day: You need to save the routes that every truck in your\\nfleet has performed each day. You can create a Materialized View with a groupArray, so you\\ncan easily return the routes per truck per day.\\nError Logs: You work with logs but just want to analyze those with the type \"error\". You can\\ncreate a Materialized View to filter by just that type.\\nReal-Time Events: You have a real-time application that doesn\\'t need historical relevance.\\nYou can create a Materialized View to filter only timestamps within the last minute.\\nChanging Indexes: You want to reuse a Data Source, but you have a new use case that\\nrequires you to filter by non-indexed columns. You can create a Materialized View and index\\nby the columns you want to filter.\\nExtract data from geolocation JSON\\x00',\n",
       "   0.6928363486086783),\n",
       "  ('15/04/2024, 14:46 Materialized Views · Tinybird Docs\\nhttps://www.tinybird.co/docs/concepts/materialized-views 1/4Materialized Views\\nWhat is a Materialized View?\\nA Materialized View in Tinybird is a combination of a Pipe and a Data Source. Instead of\\npublishing a Node as an API Endpoint, you can write the results of a query into another Data\\nSource.\\nUnlike other databases, Materialized Views in Tinybird are incremental and triggered upon\\ningestion. This means that, as you ingest new rows, data is automatically processed by the\\nmaterialization query, combined with the previous result, and written to the Materialized View.\\nBecause of this, there is no schedule to maintain for refreshing data in a Materialized View.\\nMaterialized Views are processed at ingestion time, making them an excellent way to\\npreprocess data before being used in API Endpoints. Materialized Views are critical for reducing\\nboth latency and cost-per-query.\\nWhat should I use Materialized Views for?\\nThere are three common',\n",
       "   0.6788176537381694),\n",
       "  (\"mply, Materialized Views shift computational load from query time to ingestion time, so\\nyour endpoints stay fast.\\nWhen should I use Materialized Views?\\nMaterialized Views are used to accomplish 2 main goals:\\n\\x00\\x00 Improve endpoint performance by reducing query speed and scan size\\n 15/04/2024, 14:44 Materialized Views · Tinybird Docs\\nhttps://www.tinybird.co/docs/publish/materialized-views 2/18\\x00\\x00 Simplifying query development by automating common filters andaggregations\\nAs such, consider using a Materialized View if you are experiencing any of the following:\\nA simple query taking too much time\\nYou're using the same aggregations on the same Data Source in multiple Pipes\\nA single query is processing too much data\\nYou need to change the data schema for a new use case\\nHere are a few real-world examples of when a Materialized View makes sense:\\nAggregating Sales per Product: You're ingesting real-time sales events data, but you\\nfrequently need to query the total orders or revenue by product.\",\n",
       "   0.6764457596657619),\n",
       "  (\"s allow data to be transformed from an origin data source to a destination\\ndata source. There are several scenarios where Materialized Views are really useful and can\\nmake a difference in the response times of your analyses, for instance:\\nDenormalize several normalized tables into one via a JOIN.\\nTransform data using an optimized ENGINE for a concrete analysis.\\nTransform your source data on the fly as you ingest data in your origin data source.\\nIt's important to understand that Materialized Views are live views of your origin data source.\\nAny time you append or replace data to your origin data source, all the destination data\\nsources crated as Materialized Views are properly synced. It means you don't have to worry\\nabout costly re-sync processes.\\nLet's say you have an origin data source (my_origin.datasource ) like this one:    `test3` Int64,\\n    `new_column`  Int64\\nORIGIN DATA SOURCE 15/04/2024, 14:45 CLI common use cases · Tinybird Docs\",\n",
       "   0.6669738706660705)]}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_augmented_qa_pipeline.run_pipeline(\"What is a Materialized View?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response': 'Populates can be triggered from the CLI using the `--populate` command. When using the CLI, you can specify options such as `--truncate` to truncate the materialized Data Source before populating it, or `--unlink-on-populate-error` which unlinks the Materialized View if the populate job fails. Additionally, the `--wait` option can be used to wait for populate jobs to finish, showing a progress bar.',\n",
       " 'context': [(\" concepts\\nArchitecture\\nOverview\\nData Sources API\\nEvents API\\nBigQuery connector\\nConfluent connector\\nKafka connector\\nRedPanda connector\\nSnowflake connector\\nS3 connector\\nOverview\\nSQL best practices\\nQuery parameters\\nOverview\\nAPI gateways\\nMaterialized Views\\nCopy Pipes\\nS3 Sink\\nStart building\\nSign In 15/04/2024, 14:45 Tinybird CLI command reference · Tinybird Docs\\nhttps://www.tinybird.co/docs/cli/command-ref 17/21--unlink-on-populate-error \\x00 If the populate job fails the Materialized View is unlinked\\nand new data won't be ingested in the Materialized View. First time a populate job fails, the\\nMaterialized View is always unlinked\\n--fixtures\\x00 Append fixtures to Data Sources\\n--wait\\x00 To be used along with --populate command. Waits for populate jobs to finish,\\nshowing a progress bar. Disabled by default\\n--yes\\x00 Do not ask for confirmation\\n--only-response-times \\x00 Checks only response times, when --force push a Pipe\\n--workspace TEXT..., --workspace_map TEXT... \\x00 Add a workspace path to the list of\",\n",
       "   0.48448523595784415),\n",
       "  (\"encastsChangelogStarter KitsCommunity\\nIntroduction\\nQuick start\\nCore concepts\\nArchitecture\\nOverview\\nData Sources API\\nEvents API\\nBigQuery connector\\nConfluent connector\\nKafka connector\\nRedPanda connector\\nSnowflake connector\\nS3 connector\\nOverview\\nSQL best practices\\nQuery parameters\\nOverview\\nAPI gateways\\nMaterialized Views\\nCopy Pipes\\nS3 Sink\\nStart building\\nSign In 15/04/2024, 14:45 Tinybird CLI command reference · Tinybird Docs\\nhttps://www.tinybird.co/docs/cli/command-ref 14/21COMMAND DESCRIPTION OPTIONS\\npresent. Including in the sql_condition  any colum\\npresent in the Data Source engine_sorting_key\\nwill make the populate job process less data.\\n--truncate\\x00 Truncates the materialized Data\\nSource before populating it.\\n--unlink-on-populate-error \\x00 If the populate job\\nfails the Materialized View is unlinked and new data\\nwon't be ingested in the Materialized View. First tim\\na populate job fails, the Materialized View is always\\nunlinked.\\n--wait\\x00 Waits for populate jobs to finish, showing a\",\n",
       "   0.48291944185015556),\n",
       "  (\"sing the CLI. For more\\ninformation, see the section below, and the deployment strategies docs.\\nDevelop using the CLI\\nWhen wanting to prototype a new endpoint or a change in its logic, we recommend you use the\\nUI. It is the easiest and fastest way to iterate and validate your changes, and you can see the\\nresults of your changes in real time.\\nBut when needing to make changes like data migrations or changes in column types, you need\\nto use the CLI and modify the datafiles. Make sure you're familiar with the Tinybird CLI docs.\\nFor changes like these, use the Git workflow: Create a new branch, make the changes in the\\ndatafiles, and create a Pull Request to validate the changes. For further guidance, read the\\nCI/CD docs and deployment strategies docs.\\nThis image visualizes each process step when setting up and working with Git via the Tinybird\\nCLI\\x00DocsGuidesScreencastsChangelogStarter KitsCommunity\\nIntroduction\\nQuick start\\nCore concepts\\nArchitecture\\nOverview\\nData Sources API\\nEvents API\",\n",
       "   0.46272626386207316),\n",
       "  (\" It has precedence over\\npopulate_condition\\npopulate_conditionString Optional. Populate with a SQL condition to be\\napplied to the trigger Data Source of the\\nMaterialized View. For instance,CHECK FAILED POPULATE JOBS 15/04/2024, 14:47 Pipes API · Tinybird Docs\\nhttps://www.tinybird.co/docs/api-reference/pipe-api 23/37KEY TYPE DESCRIPTION\\npopulate_condition='date  ==\\ntoYYYYMM(now())'  it’ll populate taking all the\\nrows from the trigger Data Source which\\ndate is the current month.\\npopulate_condition  is not taken into\\naccount if the populate_subset  param is\\npresent. Including in the\\npopulate_condition  any column present in\\nthe Data Source engine_sorting_key  will\\nmake the populate job process less data.\\ntruncate String Optional. Default is false. Populates over\\nexisting data, useful to populate past data\\nwhile new data is being ingested. Use true\\nto truncate the Data Source before\\npopulating.\\nunlink_on_populate_errorString Optional. Default is false. If the populate job\\nfails the\",\n",
       "   0.46106521366951003)]}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_augmented_qa_pipeline.run_pipeline(\"How are populates triggered from the CLI?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Navigate to the Weights and Biases \"run\" link to see how your LLM is performing!\n",
    "\n",
    "```\n",
    "View run at YOUR LINK HERE\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ❓QUESTION:\n",
    "\n",
    "What is the `model_name` from the WandB `root_span` trace?\n",
    "gpt-3.5-turbo-0125\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: RAG Evaluation Using GPT-4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Materialized View in Tinybird is a combination of a Pipe and a Data Source where the results of a query are written into another Data Source. Unlike traditional databases, Materialized Views in Tinybird are incremental and triggered upon data ingestion. This means that as new data is ingested, it is automatically processed by the materialization query and combined with the previous result in the Materialized View. Materialized Views are processed at ingestion time, making them a useful tool for preprocessing data before using it in API Endpoints. They help to reduce both latency and cost-per-query by shifting computational load from query time to ingestion time.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"clarity\":\"9\",\"faithfulness\":\"10\",\"correctness\":\"10\"}'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is a Materialized View?\"\n",
    "\n",
    "response = retrieval_augmented_qa_pipeline.run_pipeline(query)\n",
    "\n",
    "print(response[\"response\"])\n",
    "\n",
    "evaluator_system_template = \"\"\"You are an expert in analyzing the quality of a response.\n",
    "\n",
    "You should be hyper-critical.\n",
    "\n",
    "Provide scores (out of 10) for the following attributes:\n",
    "\n",
    "1. Clarity - how clear is the response\n",
    "2. Faithfulness - how related to the original query is the response and the provided context\n",
    "3. Correctness - was the response correct?\n",
    "\n",
    "Please take your time, and think through each item step-by-step, when you are done - please provide your response in the following JSON format:\n",
    "\n",
    "{\"clarity\" : \"score_out_of_10\", \"faithfulness\" : \"score_out_of_10\", \"correctness\" : \"score_out_of_10\"}\"\"\"\n",
    "\n",
    "evaluation_template = \"\"\"Query: {input}\n",
    "Context: {context}\n",
    "Response: {response}\"\"\"\n",
    "\n",
    "try:\n",
    "    chat_openai = ChatOpenAI(model_name=\"gpt-4-turbo\")\n",
    "except:\n",
    "    chat_openai = ChatOpenAI()\n",
    "\n",
    "evaluator_system_prompt = SystemRolePrompt(evaluator_system_template)\n",
    "evaluation_prompt = UserRolePrompt(evaluation_template)\n",
    "\n",
    "messages = [\n",
    "    evaluator_system_prompt.create_message(format=False),\n",
    "    evaluation_prompt.create_message(\n",
    "        input=query,\n",
    "        context=\"\\n\".join([context[0] for context in response[\"context\"]]),\n",
    "        response=response[\"response\"]\n",
    "    ),\n",
    "]\n",
    "\n",
    "chat_openai.run(messages, response_format={\"type\" : \"json_object\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this notebook, we've gone through the steps required to create your own simple RAQA application!\n",
    "\n",
    "Please feel free to extend this as much as you'd like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">frosty-sun-6</strong> at: <a href='https://wandb.ai/kikeap/Visibility%20Example/runs/mqiikz4x' target=\"_blank\">https://wandb.ai/kikeap/Visibility%20Example/runs/mqiikz4x</a><br/> View project at: <a href='https://wandb.ai/kikeap/Visibility%20Example' target=\"_blank\">https://wandb.ai/kikeap/Visibility%20Example</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240415_145531-mqiikz4x/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "buildyourownlangchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
